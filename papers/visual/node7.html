<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 99.1 release (March 30, 1999)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Conclusion</TITLE>
<META NAME="description" CONTENT="Conclusion">
<META NAME="keywords" CONTENT="visual">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="LaTeX2HTML v99.1 release">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="visual.css">

<LINK REL="next" HREF="node8.html">
<LINK REL="previous" HREF="node6.html">
<LINK REL="up" HREF="visual.html">
<LINK REL="next" HREF="node8.html">
</HEAD>

<BODY >

<A NAME="tex2html89"
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next_motif.gif"></A> 
<A NAME="tex2html87"
 HREF="visual.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up_motif.gif"></A> 
<A NAME="tex2html81"
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="previous_motif.gif"></A>   <BR>
<B> Next:</B> <A NAME="tex2html90"
 HREF="node8.html">Bibliography</A>
<B>Up:</B> <A NAME="tex2html88"
 HREF="visual.html">Formant Tracking in Speech</A>
<B> Previous:</B> <A NAME="tex2html82"
 HREF="node6.html">Experimental Results</A>
<BR> <P>

<!--End of Navigation Panel-->

<H1><A NAME="SECTION00050000000000000000">
Conclusion</A>
</H1>

<P>
In this paper, we have propose a simple yet 
novel formant tracking technique, which
tries to emulate the human speech cognitive model, which is not only able to
reliably extract formants in the voiced regions but also is able to identify
the formants in the unvoiced regions. 
We have through experiments demonstrated that a
formant tracking system, 
based on visual cues of a spectrogram performs as good as any formant tracking
systems in the voiced regions  but is also reliably able to extract the formant
transitions in the unvoiced area of the speech signal. In this respect, the
visual cue based formant tracking can be visualized as a superior
 alternative to conventional formant tracking systems. As can be observed, the
visual cue based formant tracking method is based on the
voiced-unvoiced decision has a significant impact on
the tracking process especially 
in the unvoiced segments, hence a more reliable voiced unvoiced region
detecting algorithm would help
better formant tracking in the unvoiced
segments. Also, the visual cues method currently does not perform well
when formants are very close to each other. Currently work is on to address
these issues.

<P>

<HR>
<A NAME="tex2html89"
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next_motif.gif"></A> 
<A NAME="tex2html87"
 HREF="visual.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up_motif.gif"></A> 
<A NAME="tex2html81"
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="previous_motif.gif"></A>   <BR>
<B> Next:</B> <A NAME="tex2html90"
 HREF="node8.html">Bibliography</A>
<B>Up:</B> <A NAME="tex2html88"
 HREF="visual.html">Formant Tracking in Speech</A>
<B> Previous:</B> <A NAME="tex2html82"
 HREF="node6.html">Experimental Results</A>

<!--End of Navigation Panel-->
<ADDRESS>
<I>Sunil Kopparapu <BR>
2002-12-20</I>
</ADDRESS>
</BODY>
</HTML>
