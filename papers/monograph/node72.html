<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with jLaTeX2HTML 2002 (1.62) JA patch-1.4
patched version by:  Kenshi Muto, Debian Project.
LaTeX2HTML 2002 (1.62),
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Properties</TITLE>
<META NAME="description" CONTENT="Properties">
<META NAME="keywords" CONTENT="kmono">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="Generator" CONTENT="jLaTeX2HTML v2002 JA patch-1.4">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="kmono.css">

<LINK REL="next" HREF="node73.html">
<LINK REL="previous" HREF="node71.html">
<LINK REL="up" HREF="node69.html">
<LINK REL="next" HREF="node73.html">
</HEAD>

<BODY >

<A NAME="tex2html1139"
  HREF="node73.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.gif"></A> 
<A NAME="tex2html1135"
  HREF="node69.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.gif"></A> 
<A NAME="tex2html1129"
  HREF="node71.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.gif"></A> 
<A NAME="tex2html1137"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/share/latex2html/icons/contents.gif"></A>  <BR>
<B> Next:</B> <A NAME="tex2html1140"
  HREF="node73.html">The Algorithm</A>
<B>Up:</B> <A NAME="tex2html1136"
  HREF="node69.html">Experiments</A>
<B> Previous:</B> <A NAME="tex2html1130"
  HREF="node71.html">Example</A>
<BR> <P>

<!--End of Navigation Panel-->

<H4><A NAME="SECTION00920030000000000000">
Properties</A>
</H4>:

<UL>
<LI>Total contribution of  for all  is , namely,
<BR><P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{displaymath}
W=\sum_{i=0}^{N-1}r_{ij}=\rho
\end{displaymath}
 -->


</DIV>
<BR CLEAR="ALL">
<P></P>
</LI>
<LI>Total contribution of  for all <IMG
 WIDTH="47" HEIGHT="34" ALIGN="BOTTOM" BORDER="0"
 SRC="img119.gif"
 ALT="$V$"> is 1, 
namely, 
<BR><P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{displaymath}
W_i=\sum_{i=0}^{M-1}r_{ij}=1
\end{displaymath}
 -->


</DIV>
<BR CLEAR="ALL">
<P></P>
</LI>
</UL>
[Proof of Theorem 4.6]Proof of Theorem&nbsp;<A HREF="node45.html#thp4">4.6</A>
<A NAME="sec:bayes_net_proof"></A>
<P>
We index the variables of the 
Bayesian network by an ordering consistent with the
orientation of the Bayesian network as <!-- MATH
 ${V} = \{V_{1}, V_{2}, \cdots 
V_{N}\}$
 -->
, namely,  is a descendant of  only if .
Then the joint pdf joint pdf of the network can be written as 
<BR>
<DIV ALIGN="CENTER"><A NAME="a1"></A>
<!-- MATH
 \begin{eqnarray}
P[V = v] &=& P[V_{1} = v_{1}, V_{2} = v_{2}, \cdots V_{N} = v_{N}] \nonumber \\
&=& P[V_{1} = v_{1}],P[V_{2} = v_{2}\mid V_{1} = v_{1}], P[V_{3} = v_{3}\mid V_{1} = v_{1}, V_{2} = v_{2}] \cdots \nonumber \\
&=& \prod_{i} P[V_{i} = v_{i}\mid V_{1} = v_{1},V_{2} = v_{2}, \cdots V_{i - 1} = v_{i - 1}] \nonumber \\
&=& \prod_{i} P[V_{i} = v_{i}|{U}_{V_{i}} = {u}_{V_{i}}]
\end{eqnarray}
 -->
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD></TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="128" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img82.gif"
 ALT="$\textstyle =$"></TD>
<TD></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="128" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img82.gif"
 ALT="$\textstyle =$"></TD>
<TD></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="128" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img82.gif"
 ALT="$\textstyle =$"></TD>
<TD></TD>
<TD WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT">&nbsp;</TD>
<TD ALIGN="CENTER" NOWRAP><IMG
 WIDTH="128" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img82.gif"
 ALT="$\textstyle =$"></TD>
<TD></TD>
<TD WIDTH=10 ALIGN="RIGHT">
(53)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL"><P></P>
Equation (<A HREF="node72.html#a1">E.1</A>) holds because the parents of a variable  are a subset of the set of its predecessors
that, once known, renders  independent of all its other 
predecessors (Theorem <A HREF="node40.html#th31">4.1</A>).
Now consider a typical variable  
having parent set  and <IMG
 WIDTH="25" HEIGHT="32" ALIGN="BOTTOM" BORDER="0"
 SRC="img118.gif"
 ALT="$L$"> children 
<!-- MATH
 ${Y}_{V_i} = \{Y_{1}, Y_{2}, \cdots Y_{m}\}$
 -->
.  appears 
exactly in  factors of the product
in (<A HREF="node72.html#a1">E.1</A>); once in <!-- MATH
 $P[V_i = v_i|{U}_{V_i} = {u}_{V_i}]$
 -->
 and once each in 
<!-- MATH
 $P[Y_{j} = y_{j}|{F}_{j} = {f}_{j}]$
 -->
 corresponding to the  
child of . Thus, we can
write
<P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{eqnarray*}
P[{V} = {v}] \!\!&=&\!\!
 P[V_i = v_i, {W}_{V_i} = {w}_{V_i}] \nonumber \\
&=&\!\! P[V_i = v_i \mid {U}_{V_i} = {u}_{V_i}]\prod_{j=1}^{L} P[Y_{j} = y_{j} \mid {F}_{j} = {f}_{j}] \nonumber \\
 & &\prod_{k \,\in \,{K}} P[V_{k} = v_{k}|{U}_{V_{k}} = {u}_{V_{k}}]
\end{eqnarray*}
 -->
<BR CLEAR="ALL"></DIV><P></P>
<BR CLEAR="ALL"><P></P>
<BR CLEAR="ALL"><P></P>
where <!-- MATH
 ${K} = \{k\mid V_{k} \in { W}_{V_i} - {Y}_{V_i}\}$
 -->
.
Since  does not figure in the last product it can be treated as a constant  with respect to
. So, we can write 
<BR><P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{displaymath}
P[V_i = v_i, {W}_{V_i} = { w}_{V_i}] = 
{\alpha}'P[V_i = v_i \mid {U}_{V_i} = {u}_{V_i}]\prod_{j=1}^{L} P[Y_{j} = y_{j} \mid {F}_{j} = {f}_{j}]
\end{displaymath}
 -->


</DIV>
<BR CLEAR="ALL">
<P></P>
Since <!-- MATH
 $P[{W}_{V_i} = {w}_{V_i}] = \sum_{v_i} P[V_i = v_i, {W}_{V_i} = {w}_{V_i}]$
 -->
, it is also a constant 
with respect to . So, we can write 
<P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{eqnarray*}
P[V_i = v_i \mid {W}_{V_i} = {w}_{V_i}] \!\!&=&\!\! \frac{P[V_i = v_i, {W}_{V_i} = {w}_{V_i}]}{P[{W}_{V_i} = {w}_{V_i}]} \nonumber \\
\!\!&=&\!\! \alpha P[V_i = v_i\mid {U}_{V_i} = {u}_{V_i}]\prod_{j=1}^{L} P[Y_{j} = y_{j}\mid {F}_{j} = {f}_{j}]
\end{eqnarray*}
 -->
<BR CLEAR="ALL"></DIV><P></P>
<BR CLEAR="ALL"><P></P>
<BR CLEAR="ALL"><P></P>

<P>
This proves the Theorem.
k-means clustering 
<A NAME="app:k_means"></A>
<P>
A technique similar to vector quantization with squared error cost
function developed my MacQueen in 1967 goes by the name k-means
clustering algorithm [<A
 HREF="node80.html#jr_Lind_80">118</A>]. The basic idea of k-means
clustering algorithm is <EM>minimum distortion</EM> (i) partitions and 
(ii) centroids. 

<P>
The goal of k-means clustering algorithm is to produce a partition  <!-- MATH
 $= \{ S_0, \cdots S_{N-1} \}$
 -->
 of the training alphabet <!-- MATH
 $A = \{x_i,
i=0, \cdots, n-1 \}$
 -->
 consisting of all vectors in the training sequence. 
The corresponding alphabet  will then be the collection of the
Euclidean centroids of the set , that is, the final reproduction
alphabet will be optimal for the final partition (but the final partition
may not be optimal for that final reproduction alphabet except when <!-- MATH
 $n
\longrightarrow \infty$
 -->
).  To obtain , we can think of each
 as a bin in which to place the training sequence vector until all
are placed.  Initially we need to decide on the bins (say ), then we
start by placing the first of the  vectors (from ) in separate bins,
, <!-- MATH
 $i=0, \cdots N-1$
 -->
. We then proceed as follows: at each
iteration, a new training vector  is observed, and the  for
which the distortion between  and the centroid <!-- MATH
 ${x(\hat{S_i})}$
 -->
 is
minimized and then the new vector  is added to the bin . Thus, at
each iteration, the new vector is added to the bin with the closest
centroid, so that the next time, this bin will have a new centroid.  This
operation is continued till all sample vectors are exhausted. 

<P>
<HR>
<A NAME="tex2html1139"
  HREF="node73.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.gif"></A> 
<A NAME="tex2html1135"
  HREF="node69.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up.gif"></A> 
<A NAME="tex2html1129"
  HREF="node71.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev.gif"></A> 
<A NAME="tex2html1137"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/share/latex2html/icons/contents.gif"></A>  <BR>
<B> Next:</B> <A NAME="tex2html1140"
  HREF="node73.html">The Algorithm</A>
<B>Up:</B> <A NAME="tex2html1136"
  HREF="node69.html">Experiments</A>
<B> Previous:</B> <A NAME="tex2html1130"
  HREF="node71.html">Example</A>

<!--End of Navigation Panel-->
<ADDRESS>
<I> <BR>
2004-02-10</I>
</ADDRESS>
</BODY>
</HTML>
